{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a97cbf1-abc4-440d-aa93-2aa2d044c16c",
   "metadata": {},
   "source": [
    "# Profanity Detection Model\n",
    "\n",
    "We will be creating a model to identify profane words and develop a system that\n",
    "can be used to check for profanity in existing text. The goal is to build a robust profanity detection system that can\n",
    "accurately flag and filter out offensive or inappropriate language.<br>\n",
    "## Tasks\n",
    "1. [Dataset Acquisition](#section1)\n",
    "2. [Data Preprocessing](#section2)\n",
    "3. [Model Development & Training](#section3)\n",
    "4. [Metrics & Evaluation](#section4)\n",
    "5. [User Input](#section5)\n",
    "6. [Testing Nameset](#section6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2cab7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## Dataset Acquisition\n",
    "Objective is to obtain a dataset that contains a variety of examples of profane words and offensive language. Should be labeled and of sufficient size. The dataset that we will be using is sourced from:<br>\n",
    "Thomas Davidson - [Hate Speech & Offensive Language](https://github.com/t-davidson/hate-speech-and-offensive-language)<br>\n",
    "It contains a list of tweets out of which some have profane language, and some which do not.<br>\n",
    "We can see a preview of some of the data being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de079b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T07:25:42.652542600Z",
     "start_time": "2023-06-08T07:25:40.168434200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24773</th>\n",
       "      <td>you niggers cheat on ya gf's? smh....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24774</th>\n",
       "      <td>you really care bout dis bitch. my dick all in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24775</th>\n",
       "      <td>you worried bout other bitches, you need me for?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24776</th>\n",
       "      <td>you're all niggers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24777</th>\n",
       "      <td>you're such a retard i hope you get type 2 dia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Class\n",
       "24773              you niggers cheat on ya gf's? smh....      1\n",
       "24774  you really care bout dis bitch. my dick all in...      1\n",
       "24775   you worried bout other bitches, you need me for?      1\n",
       "24776                                 you're all niggers      1\n",
       "24777  you're such a retard i hope you get type 2 dia...      1\n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...      1\n",
       "24779  you've gone and broke the wrong heart baby, an...      0\n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...      1\n",
       "24781              youu got wild bitches tellin you lies      1\n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = 'C:/Users/ASUS/Desktop/Profanity Check/My data.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125d3d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T07:26:38.305749500Z",
     "start_time": "2023-06-08T07:26:38.289688700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset is:  (24783, 4) \n",
      "\n",
      "Types of each column:\n",
      "Sentence           object\n",
      "Offensive           int64\n",
      "Non - offensive     int64\n",
      "Class               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Dataset is: \",df.shape,\"\\n\")\n",
    "print(\"Types of each column:\\n\",df.dtypes,sep=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a93fd8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Results\n",
    "There are 24,783 sentences in this set, out of which the following number are Profane - Not Profane<br>\n",
    "\n",
    "| Type        | Count |\n",
    "|-------------|-------|\n",
    "| Profane     | 20620 |\n",
    "| Non-Profane | 4163 |\n",
    "\n",
    "There are lots of extra unwanted characters in the data, hence we will have to preprocess the data before training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ec46c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "## Data Preprocessing\n",
    "As the dataset we are working on are tweets, we will clean the data by removing extra characters,punctuations etc.\n",
    "\n",
    "#### Soundex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d53256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T10:22:23.043908500Z",
     "start_time": "2023-06-08T10:22:23.031956600Z"
    }
   },
   "outputs": [],
   "source": [
    "def soundex(token):\n",
    "\n",
    "    # Convert the word to upper case\n",
    "    token = token.upper()\n",
    "    soundexx = \"\"\n",
    "\n",
    "    # Retain the First Letter\n",
    "    soundexx += token[0]\n",
    "\n",
    "    # Create a dictionary which maps letters to respective soundex\n",
    "    # codes. Vowels and 'H', 'W' and 'Y' will be represented by '.'\n",
    "    dictionary = {\"BFPV\": \"1\", \"CGJKQSXZ\": \"2\",\n",
    "                  \"DT\": \"3\",\n",
    "                  \"L\": \"4\", \"MN\": \"5\", \"R\": \"6\",\n",
    "                  \"AEIOUHWY\": \".\"}\n",
    "\n",
    "    # Encode as per the dictionary\n",
    "    for char in token[1:]:\n",
    "        for key in dictionary.keys():\n",
    "            if char in key:\n",
    "                code = dictionary[key]\n",
    "                if code != '.':\n",
    "                    if code != soundexx[-1]:\n",
    "                        soundexx += code\n",
    "\n",
    "    # Trim or Pad to make Soundex a\n",
    "    # 7-character code\n",
    "    soundexx= soundexx[:4].ljust(4, \"0\")\n",
    "    # im making it 4, change 4 to 7 if req\n",
    "    return soundexx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ab73fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T11:58:36.105867600Z",
     "start_time": "2023-06-08T11:58:32.523063700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  Class  \\\n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...      0   \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1   \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1   \n",
      "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...      1   \n",
      "6  !!!!!!\"@__BrighterDays: I can not just sit up ...      1   \n",
      "7  !!!!&#8220;@selfiequeenbri: cause I'm tired of...      1   \n",
      "8  \" &amp; you might not get ya bitch back &amp; ...      1   \n",
      "9  \" @rhythmixx_ :hobbies include: fighting Maria...      1   \n",
      "\n",
      "                                      clean_Sentence  \n",
      "0  mayasolovely woman complain clean house amp ma...  \n",
      "1    boy dats cold tyga dwn bad cuffin dat hoe place  \n",
      "2  urkindofbrand dawg you ever fuck bitch start c...  \n",
      "3                                   look like tranny  \n",
      "4  shenikaroberts the shit hear might true might ...  \n",
      "5  the shit blow claim faithful somebody still fu...  \n",
      "6            sit hate another bitch get much shit go  \n",
      "7  selfiequeenbri cause tire big bitch come skinn...  \n",
      "8                 amp might get bitch back amp thats  \n",
      "9                 hobbies include fight mariam bitch  \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sen = df[\"Sentence\"]\n",
    "\n",
    "\n",
    "#Defining a lemmatizer\n",
    "#Removes stop words, words with symbols and words of length < 3\n",
    "#Also converts all words to lower case\n",
    "def lemmatize_words(text):\n",
    "       words = word_tokenize(text)\n",
    "       words = [(lemmatizer.lemmatize(word,pos='v')).lower() for word in words if not word in stop_words if len(word)>=3 if word.isalpha()]\n",
    "       return ' '.join(words)\n",
    "\n",
    "\n",
    "# Soundex prototype 1\n",
    "# Pls delete and uncomment above thingy\n",
    "# def lemmatize_words(text):\n",
    "#        words = word_tokenize(text)\n",
    "#        words = [soundex(lemmatizer.lemmatize(word,pos='v')) for word in words if not word in stop_words if len(word)>=3 if word.isalpha()]\n",
    "#        return ' '.join(words)\n",
    "\n",
    "df[\"clean_Sentence\"] = sen.apply(lemmatize_words)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641112c2",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## Model Development\n",
    "\n",
    "The data has been cleaned, now we proceed to import the required libraries and start training the model.<br>\n",
    "The classification will be done using the Support Vector Machine (SVM) model and the Logistic Regression model and the more accurate one will be used in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63b7131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T12:08:17.789834900Z",
     "start_time": "2023-06-08T12:08:17.777832300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6716c",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b928a3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T07:27:49.453573300Z",
     "start_time": "2023-06-12T07:27:47.581535200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(sen)\n",
    "y = np.array(df[\"Class\"])\n",
    "\n",
    "#Vectorizing the text\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(x)\n",
    "# print(x)\n",
    "\n",
    "\n",
    "#Splitting the Dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.33, random_state= 0, shuffle= False)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1d155",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5852c758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T12:06:16.359634500Z",
     "start_time": "2023-06-08T12:06:16.036711500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(sen)\n",
    "y = np.array(df[\"Class\"])\n",
    "\n",
    "#Vectorizing the text\n",
    "vect = TfidfVectorizer()\n",
    "xx = vect.fit_transform(x)\n",
    "\n",
    "#Splitting the Dataset\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(xx,y, test_size = 0.33, random_state= 0, shuffle= False)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(xx_train,yy_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084a2e7",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"section4\"></a>\n",
    "## Metrics & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad405d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T07:28:05.833677Z",
     "start_time": "2023-06-12T07:28:05.800604600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for SVM Model: \n",
      "\n",
      "Accuracy: 0.9543954028609855\n",
      "Precision: 0.9772054470100652\n",
      "Recall: 0.9678932707814103\n",
      "F1 Score: 0.9725270678353097\n",
      "Confusion Matrix:\n",
      " [[1204  154]\n",
      " [ 219 6602]]\n"
     ]
    }
   ],
   "source": [
    "## SVM MODEL\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Metrics for SVM Model: \\n\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\",f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708acb1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T12:09:49.218033500Z",
     "start_time": "2023-06-08T12:09:49.177019300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Logistic Regression: \n",
      "\n",
      "Accuracy: 0.9402127399437584\n",
      "Precision: 0.9595065312046445\n",
      "Recall: 0.9692127254068318\n",
      "F1 Score: 0.9643352053096055\n",
      "Confusion Matrix:\n",
      " [[1079  279]\n",
      " [ 210 6611]]\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "\n",
    "yy_pred = logreg.predict(x_test)\n",
    "print(\"Metrics for Logistic Regression: \\n\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test, yy_pred))\n",
    "print(\"Precision:\",precision_score(y_test, yy_pred))\n",
    "print(\"Recall:\",recall_score(y_test, yy_pred))\n",
    "print(\"F1 Score:\",f1_score(y_test, yy_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, yy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b14410",
   "metadata": {},
   "source": [
    "Since the Accuracy, and F1 score both are higher for SVM than Logistic Regression, from now on we will be using the SVM model for testing data.<br>\n",
    "We can also look at the falsely predicted values if required to try to notice any pattern in where the model is making mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2764b5",
   "metadata": {},
   "source": [
    "#### FP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a4a3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8230 always and began boated clam flow go good had hallmarknostracollecti juicy little man mary nostradamnisuck of rt she to touch was\n",
      "\n",
      "\n",
      "any in is it now_thats_fresh rt song trash tyga with\n",
      "\n",
      "\n",
      "127480 127482 badass bald bird change eagle if logo merica ourgreatamerica rt should the think to twitter you\n",
      "\n",
      "\n",
      "all cowboys poonsoaker rt so trash\n",
      "\n",
      "\n",
      "999 bird boy done flappy im in life poisonedkissx3 reached rt so this white with\n",
      "\n",
      "\n",
      "co girls have how http jprbhuxfdp love pornpunter pussies rt sweet\n",
      "\n",
      "\n",
      "8230 at em has https in just of office on or out politicians rt run rwsurfergirl the throw toss trash trashbucketchallenge\n",
      "\n",
      "\n",
      "all dam dental gotta have is luuuube luuuuube need racheldoesstuff really rt ya yankees you\n",
      "\n",
      "\n",
      "co http of pic posing rakwonogod rare rt trash usgiazsdwv with\n",
      "\n",
      "\n",
      "8230 99 be bring food from ghetto girl if into might movies no outside pay ratchet2english rt stupid the tweeted you\n",
      "\n",
      "\n",
      "aight co dykes game had http it o0can6gb1p over rihannahasaids rt ruin to\n",
      "\n",
      "\n",
      "8230 99 be bring food from ghetto girl if into might movies no outside pay rontheanchorman rt stupid the tweeted you\n",
      "\n",
      "\n",
      "charlie foxtrot kilo rt shawnzie3 uniform\n",
      "\n",
      "\n",
      "butt chug hard lemonade lets mikes rt shitpussiessay\n",
      "\n",
      "\n",
      "all cards co gi http into lately oh pussies rt sfvcndhwzv shitpussiessay themselves these turning what with yu\n",
      "\n",
      "\n",
      "birthday cotton happy little my picker rt shinsnipes snipersinperil\n",
      "\n",
      "\n",
      "8230 another as booty cool dancing fellas girl her in mans nig on other pacdagoat penis rt rubbing solodahsystem sure with you your\n",
      "\n",
      "\n",
      "8230 ahead and cadillac day drive enough gay go kay ll make mary me mock of one pink rt see sell soulyodeler the this to up we who\n",
      "\n",
      "\n",
      "8230 bro brownies but crazy da elves going gone have in it keebler legalized make rt shi southsidevic that to trap tree trunk weed when\n",
      "\n",
      "\n",
      "are around be because embarrassed honestly how if like look of re rt someone stealurgirlbush they to trash you\n",
      "\n",
      "\n",
      "co ghetto http re rt stevestfler stop talking white wzbdbi0f9k you\n",
      "\n",
      "\n",
      "128077 8230 and be bung desperate don example for good guys happy here holes negativity not of on rt spew tera1patrick the to what\n",
      "\n",
      "\n",
      "127770 128514 8220 8221 bird blaxxxican dick early gets rt tattedup_tre the\n",
      "\n",
      "\n",
      "8230 amp and chilling ever have just ladies like nips of on one or outside rt shirt something tank tazheadorn the top was yall you your\n",
      "\n",
      "\n",
      "8230 99 be bring food for from ghetto girl if into might movies no outside pay rt stupid tbhmoose the tweeted you\n",
      "\n",
      "\n",
      "all atoma87 co fags http lmao mn0tf3dgo7 rt some thatboyado tonykivarkis\n",
      "\n",
      "\n",
      "8217 and five nipples real rt secret sideways thegoodgodabove vagina victoria\n",
      "\n",
      "\n",
      "bud dude flighty he if in is it just nip probably rt seems so the then thenaimaw\n",
      "\n",
      "\n",
      "128563 1dittlinger all can down drunks get gravity has it lol over redneck rt that to tomolson8 use written\n",
      "\n",
      "\n",
      "8230 about and around baffles be close do how it me people probably rt smh someone talk the then tooeffincool trash turn upmost\n",
      "\n",
      "\n",
      "8230 asked black boy do hey is like man me niggers personally rt said st that tooracist use well white word wouldn you\n",
      "\n",
      "\n",
      "because called color farrakhan he is jesus knew monkey not reason rt the thetime toyamiani was you your\n",
      "\n",
      "\n",
      "cinco cm9jhhk41y co de ho http rt tweetlikeagiri way\n",
      "\n",
      "\n",
      "co dyke elbasedparko ended hate he http just like looks nah now people racism rt since think unorthodex vgjyhn4bvt white\n",
      "\n",
      "\n",
      "at back better doesn ex gemini in just look really rt some the their things trash usgeminis\n",
      "\n",
      "\n",
      "alien co crime cspanwj http illegal is it jeandeaux1776 mcdonalds muchomacho not pf2ypelyop race rt vatxn wetbacks\n",
      "\n",
      "\n",
      "are determine nipples not or rt the titties trash waynel_jr whether\n",
      "\n",
      "\n",
      "8230 asshole at away be bed beef cause from go going gotta my redneck rt stepping the to twitter up us whiteboicoleman\n",
      "\n",
      "\n",
      "aw broke crap first it oreos rt then twist wolfpackalan you\n",
      "\n",
      "\n",
      "15 8230 aged ago and at beaners by chimpout got great his metlife middle old rt salvadoran saw son summers the wasted woody_afc yr\n",
      "\n",
      "\n",
      "bunch co d5dphkwi2l group http in of puts retards rt teacher when with you your yourfavposts\n",
      "\n",
      "\n",
      "all are black ghetto girls not rt zephthegreat\n",
      "\n",
      "\n",
      "_jbsoprano act all and bad be broads grew hard jaetips rt the to trash up young\n",
      "\n",
      "\n",
      "_johnfkennedy_ another fact is man one rt trash wife\n",
      "\n",
      "\n",
      "10 _moonwp_ before co dykes free hear http ladies ptk6czttks rt when\n",
      "\n",
      "\n",
      "8220 8221 8230 ______0__o_____ _ashwhole blunt co dee hits hoe https ihitmodelsraw rt she so terrylee__ this thought was wiqgndxgnm\n",
      "\n",
      "\n",
      "also amazingatheist america around but drunken fists flailing funny he his incoherently is kind like of raving redneck rt you\n",
      "\n",
      "\n",
      "am anarchistrev as claim cry just like longer my no queer rallying rt to willing you\n",
      "\n",
      "\n",
      "anticvlum eating noneshallpass97 rt trash\n",
      "\n",
      "\n",
      "bclaymoore does dude explainafilmplotbadly folk girlfriend her his mom mopes rock rt to trashes wedding\n",
      "\n",
      "\n",
      "alive bniceloco charlie co http is nigga pk0nejiovh realest rt sheen the\n",
      "\n",
      "\n",
      "america and be boburnham by considered cool culture future gay in instead of rt straggots teens the there wiggers will\n",
      "\n",
      "\n",
      "8220 8221 better camxo__ is playstation rosaalbae rt titties trash xbox ya\n",
      "\n",
      "\n",
      "8216 8217 apple as being campaign capflowwatch ceo co from gay gift god http iphone is lze2e0wuyv of part queers rt says sell to\n",
      "\n",
      "\n",
      "and answer black cigarsnscotch going is not rt the this tiger to what win woods wrong year yellow\n",
      "\n",
      "\n",
      "all are babies beanies because born clurfogarty come hipsters hospital in of out rt the they\n",
      "\n",
      "\n",
      "58385 58386 about and choking coughing danielleekendra fuckkk lol omg rt saw talk the tweet was wiggas\n",
      "\n",
      "\n",
      "8230 cash co could david_marchese didn down gambler gmk guess he ho http johnny know mind never rt say the to turned when you\n",
      "\n",
      "\n",
      "cats divonte_photos dog house inside momma my no of pussy rt said\n",
      "\n",
      "\n",
      "8230 about ah and boy burnt can cracker dril it its just lu my opens out oven perfect pulls ready ritz rt smell sshitty the up\n",
      "\n",
      "\n",
      "_whiteponyjr_ believe buckm00se cripple elchavaloko fuckn gonna hippo me or rt saucygyro so you\n",
      "\n",
      "\n",
      "8230 accountants anal are bangers doc domin enjoying erdikken fads fake girbaud jncos martens now posers rt sex they where wiggers\n",
      "\n",
      "\n",
      "8230 amp before books cooked covering creepy exjon for freddie http irs john koskinen leprechaun little mac misdeeds rt teamsters the up\n",
      "\n",
      "\n",
      "8230 99 ba be bring femaies food for from ghetto girl if into might movies no outside pay rt stupid the tweeted you\n",
      "\n",
      "\n",
      "after as bad breaking dexter died ead especially fat_rell iaintgottarap just kayla_applebum rita rt trash was\n",
      "\n",
      "\n",
      "128530 all are black ghetto girls igottfanss not rt\n",
      "\n",
      "\n",
      "1weypg8rer big but co comes got http iroib nudes rt she the they through titties trash when with\n",
      "\n",
      "\n",
      "about be became compton ghetto half how in ireland_roseee it of rt savanigga will you\n",
      "\n",
      "\n",
      "2parasites 8230 amp bankrupted by catering co driven ds ee2l5 forged ghettos http ideology jjauthor lib rt self serving tcot unions us\n",
      "\n",
      "\n",
      "8230 amp bankrupted by catering dems driven forged ghettos http ideology jjauthor liberal parasites rt self serving tcot to unions us\n",
      "\n",
      "\n",
      "and bird blue boobs change dump it just little llvvzz mascot of officially rt set should the to twitter\n",
      "\n",
      "\n",
      "8230 and as bill coming dollar first her im lovesexdeath_ my never out queer really response rt sarcastic three to tonyjrodriguez was wow\n",
      "\n",
      "\n",
      "against attacks be believe citizens death for mediacrooks ne people racial rt sentenced slurs the time to uncivilised uncouth\n",
      "\n",
      "\n",
      "8230 a1 barbeeequeee bout cheese cottage day eating ghetto in know lil livin mitch77c peppa rt salt the times ueon whatchu\n",
      "\n",
      "\n",
      "clam cock jam mtchll out rock rt with your\n",
      "\n",
      "\n",
      "8230 celebrate family father first his incitement into last murder netanyahu nig night of pm rt the this to translated traveling was\n",
      "\n",
      "\n",
      "8230 an asshole be because being blind can ever far favorite has ho line made me must my northsidegreg opieradio rt so suck to you\n",
      "\n",
      "\n",
      "class from jeter quirkisms rod rt to trash\n",
      "\n",
      "\n",
      "8220 8221 co dj5kvo7rsr http rt shaelynspacyyy they though trash uglyassderrick\n",
      "\n",
      "\n",
      "128530 8230 at bout duh for it kids momma more my ok parents party pop pussy rt shonni__ talm tf than their these think to why would you\n",
      "\n",
      "\n",
      "checking for in legs now pissed rt shower skankymunter yellow\n",
      "\n",
      "\n",
      "128514 8230 back bit blaccstone boy bury clown dare is music nurse rt say slimthugga something trash you your\n",
      "\n",
      "\n",
      "and clam cock gonna jam my out re rock rt suckdisdpiece69 with you your\n",
      "\n",
      "\n",
      "9825 awwww lot means much my niggar rt so sure t_ndyy tanaka_callen thank you\n",
      "\n",
      "\n",
      "are co http me nx2tqimo2a rt trash trentlilweezy you\n",
      "\n",
      "\n",
      "can charles charlie difficult fiancee have honeymoon manson planning rt surf their time twayne1010 will\n",
      "\n",
      "\n",
      "bitchs don every face gets go know me on pancake rt she that tight time twitter vackra_lognn why\n",
      "\n",
      "\n",
      "8230 amp bidens business call conference crackers id if joe just live mean neck obama on press rt show slit these tv veeveeveeveevee was\n",
      "\n",
      "\n",
      "by drake god is rt thank trash vodkapapixo weed_cloudz you\n",
      "\n",
      "\n",
      "8220 8221 aaadontplayy co hicks https kna2u0d2o0 lmfao rt whodatth0\n",
      "\n",
      "\n",
      "blaxican notoriousbeep rt xtruthseekerx_\n",
      "\n",
      "\n",
      "free rt wop yungkatana\n",
      "\n",
      "\n",
      "02e1ceoe3z 7old2edb1r 8221 all bombing by carpet co commence executed from http in iraq jihadis rt soldiers this tunis video\n",
      "\n",
      "\n",
      "african agree an barackobama belongs co http if in like looks monkey northkorea rofl rt says toov1uty6h who you zoo\n",
      "\n",
      "\n",
      "and anthony barnicle clown come do for good is mj morningjoe msnbc oj on or ratings really rich show teabaggers the want weiner you\n",
      "\n",
      "\n",
      "ain all and eating get gonna higher hips its nips otherwise rise up\n",
      "\n",
      "\n",
      "are bad catholics europe give in name roman the they trash us\n",
      "\n",
      "\n",
      "hibbert it roy so trash unbelievable\n",
      "\n",
      "\n",
      "ace boon campus few girls my of on one real the to yeahh_doe youmynigga\n",
      "\n",
      "\n",
      "160 36cvhp6c70 as big co cock deep her http in legs sakurako she spread takes twat wide\n",
      "\n",
      "\n",
      "and bunch failed get handful having hostess idea jokes no obama of people rehashing same search terms the twinkie while why\n",
      "\n",
      "\n",
      "6chtqnr3mu co guns http need pussies see their these why\n",
      "\n",
      "\n",
      "an anchor baby back born came canadian from go is natural senator tedcruz where you\n",
      "\n",
      "\n",
      "128073 8221 co elected http just like men oh old racist sexist sheeple sweetpotatertot teabaggers these those two white who x3bwnm3jii\n",
      "\n",
      "\n",
      "be dad ghetto grandfather great guess he in man morals my not or poor so standards tho to want was\n",
      "\n",
      "\n",
      "act and cause certain chooses hip his hop if interests is it listen not of race so someone term the to way wigger\n",
      "\n",
      "\n",
      "bail be deserve don hood jail money negros of some the these they visiting way\n",
      "\n",
      "\n",
      "colored sometime wasn wish\n",
      "\n",
      "\n",
      "ally can ceiling floor hollywood in mom near on outside sometimes somewhere the trash up wake your\n",
      "\n",
      "\n",
      "airlines before cousins driving huff in is like love mini paint southwest thinner to trash van white who with working your\n",
      "\n",
      "\n",
      "allowed any are be breed european filthy half native no should speak spics to trash\n",
      "\n",
      "\n",
      "confused hookers king last night ordered party still the to tranny two why\n",
      "\n",
      "\n",
      "0malbndkn9 be co down honky http stop to trying\n",
      "\n",
      "\n",
      "redskin stop the using word\n",
      "\n",
      "\n",
      "are even fools future ghetto ignorant niggaz no on or people read same smack stupid surprised talking the tim tom uncle write\n",
      "\n",
      "\n",
      "balls monkey my suck super\n",
      "\n",
      "\n",
      "all ce co fags foh g8nrvdmxqh gags gxxd http ll lml nd niggaz stay swag swagged up wanna who\n",
      "\n",
      "\n",
      "160 8ye0rtk7xc all and co even fists gets her holes http own penetrated teen twat\n",
      "\n",
      "\n",
      "chink co eyes fam http ignore shaylay some thanksgiving the uyvc2qdg with\n",
      "\n",
      "\n",
      "8tzvbbzxa1 co darkskinned from fucked funnyjulius ho http me mulatto rt that this to up went\n",
      "\n",
      "\n",
      "all annoying bastards english god killed leprechauns little thank the\n",
      "\n",
      "\n",
      "coast full if is midwest of south the trash west white\n",
      "\n",
      "\n",
      "bumblebees homosexual like look new steelers the uniforms wtf\n",
      "\n",
      "\n",
      "amp birds brings desire feel fills flesh head hot let me morning my of on sexual singing spring taste the this thoughts you\n",
      "\n",
      "\n",
      "browns easy first for half is literally of retarded schedule the year\n",
      "\n",
      "\n",
      "arrow chinks co dog east food http hunting in q1kqt6stwl really rt shot the their with wwltv\n",
      "\n",
      "\n",
      "cheated colored day eliminated ex from is jeans me my on same she skinny sober the told wardrobe was when\n",
      "\n",
      "\n",
      "co cracker cycdbhbxkq flip gave have he http never realdjtv sees should they this transmitter when will\n",
      "\n",
      "\n",
      "about getting my nips pierced thinking\n",
      "\n",
      "\n",
      "chick colored eyebrows her like looks pencil she straight thatshitdoesntlookgood this to took up\n",
      "\n",
      "\n",
      "bird co fuckin http q2kjv91kiz this\n",
      "\n",
      "\n",
      "9eagkxdgh5 co ghetto how http is know lhs this you\n",
      "\n",
      "\n",
      "128514 all and at bastard comes die do gonna his it just like little me rap say spinelli the thinh this time to try unto wigger\n",
      "\n",
      "\n",
      "beeotch chit co http monkey my oomruo1obi shake that this was\n",
      "\n",
      "\n",
      "ho my necks now those\n",
      "\n",
      "\n",
      "and balls co feel good grass http j0kcejqd0j love on ops other sexy shaft smooth soles spy stains the they today would\n",
      "\n",
      "\n",
      "baeemot812 big co cock country feet guy his http hv lick lv my on ops soles spy to todays with\n",
      "\n",
      "\n",
      "ago bevo bullshittin charlie is months next told wasn ya\n",
      "\n",
      "\n",
      "200 allahpundit been crookedwren germanshepher10 have in jammiewf jihadis knives our pain sharpening tolerance what years\n",
      "\n",
      "\n",
      "8220 8221 amp and at black coons dubstep expand hip hop house need only parties people play some their tiemyduragboo to trash yall\n",
      "\n",
      "\n",
      "128514 by capitalized how in is kelly misskai__ oreo rt see song tf the this trash wtf\n",
      "\n",
      "\n",
      "as ate child is maxon_maxoff5 snow truth yellow\n",
      "\n",
      "\n",
      "9zwr3o33iu attorney co divorced enduring family florida for general http is marriage reserved says stable teabagger twice units wtf\n",
      "\n",
      "\n",
      "10084 128536 co dkiqodlaop honkie http mflury30 strong this two with years\n",
      "\n",
      "\n",
      "all and apparently crap fairy happy it made my on so the took twitter ummm\n",
      "\n",
      "\n",
      "co cyc9madjpq http hypocrite tom uncle\n",
      "\n",
      "\n",
      "are birds chirping damn early smh still the too up\n",
      "\n",
      "\n",
      "be boy breaking dildo feel fu guns hey honkey hows it joe needs oath out prick routed still the to vp wants whippin your\n",
      "\n",
      "\n",
      "and blacks co cry emluwbbogw http the watch whiggers\n",
      "\n",
      "\n",
      "dawg hate lil lol nicca redfernave rt steelers the too we you\n",
      "\n",
      "\n",
      "trash we\n",
      "\n",
      "\n",
      "arab axin be christian colored donts jews know order peckin porto ricans the what white\n",
      "\n",
      "\n",
      "and bitch cheat cry do fight for go gonna her if kick no out she straight that to tootsies what you\n",
      "\n",
      "\n",
      "be dat for fukn nicca real riding rocko4real who wit\n",
      "\n",
      "\n",
      "are does have in interested morningjoe muslim oil people saving so syria teabaggers who why\n",
      "\n",
      "\n",
      "barrel chick cracker everytime girl go great is it its nope seats teeth that the tits to waitress who why with you your\n",
      "\n",
      "\n",
      "are jiggaboo you\n",
      "\n",
      "\n",
      "8230 about ain but capuano chit co cousins gram http ipynkaqufz jason nd on pussies see taking that the these two you\n",
      "\n",
      "\n",
      "gook it state you\n",
      "\n",
      "\n",
      "basketball court edgewood now ome23y rt tho trash you\n",
      "\n",
      "\n",
      "amp been booth bubble chief co don gem going gum http in it killing lkjnb7uzmt music no on real that the trash yo young\n",
      "\n",
      "\n",
      "af always barbershop bring cool ho okay the they though to usually your\n",
      "\n",
      "\n",
      "basement bastards diving mook\n",
      "\n",
      "\n",
      "4lokos butt chug nah or\n",
      "\n",
      "\n",
      "chill crackers getting im rn zahra\n",
      "\n",
      "\n",
      "127800 128150 co fuckin http tbqjhev08b twats\n",
      "\n",
      "\n",
      "bird but co consider douglasrichardsonships geekylesbianscientists http kayla okay theblairbetch this though zahyxtavvo\n",
      "\n",
      "\n",
      "bro co happy ho http husband kag0xfdd1s key kknd87mv4b marriage of so study thinks to\n",
      "\n",
      "\n",
      "at best bragging but capitalism cash got haggling hey ho im indiegamepickuplines know like not receatter that the you\n",
      "\n",
      "\n",
      "duck freebees have here hm if in life like love pom so to uckin\n",
      "\n",
      "\n",
      "im trash weeaboo\n",
      "\n",
      "\n",
      "am and border born but cause don folks follow here hopper in just life like look mean one online raised real sorry that\n",
      "\n",
      "\n",
      "casual co family ghetto http jrmepckcqs just session twerk\n",
      "\n",
      "\n",
      "doritos lmao madre nacho rt sirrocobama trash tu waymothegod\n",
      "\n",
      "\n",
      "at co ghetto girls haslmb1dzj https in lmaooooo lunch rt seat sirasshole sit when you\n",
      "\n",
      "\n",
      "album az5950 double liked lol rt smh so son that trash up was\n",
      "\n",
      "\n",
      "and best caghblkaxq co do goodnight hillbilly http is like looking losers matching not what\n",
      "\n",
      "\n",
      "asshat be because brothers friends he it my okay our property racist redneck think to vandalize wana\n",
      "\n",
      "\n",
      "_justspeedy _sirhampton_ barack_obutta candy co corn hard http lmfao no rt sqvyksdkwu these tho those trash\n",
      "\n",
      "\n",
      "asks big brownies dingbat eat else goes here jackass ofcourse over someone surprise to what when\n",
      "\n",
      "\n",
      "8220 8221 corey_emanuel driving gonna gotta her ignore just keep ky lifeasking oh say she this thru trash you\n",
      "\n",
      "\n",
      "but can care girls her how if many nips okay people really see shirt through you\n",
      "\n",
      "\n",
      "ew get lesbians nips only pierced their\n",
      "\n",
      "\n",
      "a2rbvebeg1 co http pure trash\n",
      "\n",
      "\n",
      "bone but hairlessss her pusssy red\n",
      "\n",
      "\n",
      "absoluteyvile albino coolcunts dwarf lesbo player slut specky wannabe\n",
      "\n",
      "\n",
      "10 at be card delivered goin gonna ho it like lunch one only ordering ppl say so the this to we wit work\n",
      "\n",
      "\n",
      "co http still trash x0ghhf0imn\n",
      "\n",
      "\n",
      "an co her http like mpajkwjbzp oreo stuffed\n",
      "\n",
      "\n",
      "bad can even genesis is it live mock my of so the tna tweet watching\n",
      "\n",
      "\n",
      "128525 bae gettin her nips pierced the\n",
      "\n",
      "\n",
      "because but cracker do double even hair hate have look me more not now only poc probably technically they when white\n",
      "\n",
      "\n",
      "another lmaooooooooooo monkey shot they\n",
      "\n",
      "\n",
      "6kn7pztgl8 co http trash ur\n",
      "\n",
      "\n",
      "8230 ain crunk dead ho informed just lol okay sure that was\n",
      "\n",
      "\n",
      "and at cheeto coleslaw dust fun lick nips off the thejsimps to trying twins where your\n",
      "\n",
      "\n",
      "bird just me on shitted wow\n",
      "\n",
      "\n",
      "eurotrash good manbooty shapely yes\n",
      "\n",
      "\n",
      "20_pearls 8220 8221 8230 and bible corey_emanuel his hymns in is lie lifeasking mine muthaf now right scriptures tl trash you\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "False_pos=[]\n",
    "False_neg=[]\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "       if (y_test[i]==1 and y_pred[i]==0):\n",
    "              False_neg.append(x_test[i])\n",
    "              # print(x_test[i])\n",
    "       if (y_test[i]==0 and y_pred[i]==1):\n",
    "              False_pos.append(x_test[i])\n",
    "       #        print(x_test[i])\n",
    "\n",
    "\n",
    "for j in False_neg:\n",
    "       k = cv.inverse_transform(j)\n",
    "       print(\" \".join(k[0]))\n",
    "       print(\"\\n\")\n",
    "\n",
    "# for j in False_pos:\n",
    "#        k = cv.inverse_transform(j)\n",
    "#        print(\" \".join(k[0]))\n",
    "#        print(\"\\n\")\n",
    "\n",
    "# For LogReg, replace y_test with yy_test\n",
    "# Replace cv with logreg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ec03e",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "## Input test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b48fd809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T08:43:42.963821700Z",
     "start_time": "2023-06-12T08:43:39.933259200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence: bhosdike\n",
      "Profane\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(x):\n",
    "\n",
    "    x = x.lower()\n",
    "    #Replacing symbols\n",
    "    # replacements = {'$':'s', '@':'a', '4':'a', '8':'b', '3':'e', '1':'i', '0':'o', '5':'s', '7':'t' }\n",
    "    # x = ''.join([replacements.get(char,char) for char in x])\n",
    "\n",
    "    y = cv.transform([x]).toarray()\n",
    "    ans =  clf.predict(y)\n",
    "\n",
    "    bool, booli = False, True\n",
    "    for k in x.split():\n",
    "        if (k.lower() in ex):\n",
    "            booli = False\n",
    "        if (k in hi):\n",
    "            bool = True\n",
    "    if ((ans and booli) or bool):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "a = input(\"Enter the sentence: \")\n",
    "ans = test(a)\n",
    "\n",
    "print(\"Profane\") if (ans==1) else print(\"Not Profane\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97acbca1",
   "metadata": {},
   "source": [
    "<a id=\"section6\"></a>\n",
    "## Testing Nameset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5dda21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T07:18:31.255139100Z",
     "start_time": "2023-06-12T07:18:08.372390300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raj Junior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Srijan Sinha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jagdish Patel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Raju Madur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahaan Khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nabila Shaikh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kabir Sud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aditya Bhattacharyya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mirtunjay  Kumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gulshan Kumar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Names\n",
       "0           Raj Junior \n",
       "1          Srijan Sinha\n",
       "2         Jagdish Patel\n",
       "3            Raju Madur\n",
       "4            Ahaan Khan\n",
       "5         Nabila Shaikh\n",
       "6             Kabir Sud\n",
       "7  Aditya Bhattacharyya\n",
       "8      Mirtunjay  Kumar\n",
       "9         Gulshan Kumar"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_ = 'C:/Users/ASUS/Desktop/Profanity Check/Name Set.xlsx'\n",
    "tf = pd.read_excel(excel_)\n",
    "\n",
    "tf.head(10)\n",
    "names = tf['Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3675e812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T07:20:43.018422900Z",
     "start_time": "2023-06-12T07:20:39.739520100Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(names)\n",
    "X = X.astype(str)\n",
    "\n",
    "#Vectorizing the text\n",
    "X = cv.fit_transform(X)\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in names:\n",
    "    out = test(i)\n",
    "    bool, booli = False, True\n",
    "    # if (out==1):\n",
    "    #     print(i)\n",
    "    for k in i.split():\n",
    "        if (k.lower() in ex):\n",
    "            booli = False\n",
    "        if (k in hi):\n",
    "            bool = True\n",
    "    if ((out and booli) or bool):\n",
    "        print(i)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "YP = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab496e",
   "metadata": {},
   "source": [
    "##### Loading Hindi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1caf400b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T08:38:00.065278600Z",
     "start_time": "2023-06-12T08:38:00.042268400Z"
    }
   },
   "outputs": [],
   "source": [
    "h = 'C:/Users/ASUS/Desktop/Profanity Check/Hinglish_Profanity_List.csv'\n",
    "hin = pd.read_csv(h)\n",
    "hin = hin['badir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61dcdd0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T08:38:02.586031200Z",
     "start_time": "2023-06-12T08:38:02.560428700Z"
    }
   },
   "outputs": [],
   "source": [
    "hi = hin.values.tolist()\n",
    "ex = ['deep','ms','folks', 'smile','aware', 'da', 'mutha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e1ed941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T08:40:55.637531700Z",
     "start_time": "2023-06-12T08:38:06.238252300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manoranjan Shit\n",
      "subhash  jaat\n",
      "Ho Ok Ooyyluc Hchchcchku\n",
      "Bhanwarlal jaat\n",
      "Bhanwarlal jaat\n",
      "Bhai Ho\n",
      "Babulal Godara jaat\n",
      "Pavan jaat\n",
      "keethi kutti\n",
      "Sayan Shit\n",
      "niku chutia\n",
      "Akash Darkie\n",
      "Dilip  Omg\n",
      "Sanu Shit\n",
      "OMG Tiub OMG Tiub\n",
      "Tej Ho  Joshi\n",
      "WHITE  DEVIL \n",
      "Santanu Shit\n",
      "Omg Gupta\n",
      "Black White\n",
      "Bhabani Sankar  Shit\n",
      "Sanket jaat\n",
      "Deepanshu  joon\n",
      "White Devil\n",
      "Santu Kumar Shit\n",
      "Shraban  Shit\n",
      "Rahul Tried\n",
      "Omg Godghase\n",
      "abid anus abid anus\n",
      "abid anus abid anus\n",
      "Homework Revice\n",
      "Hell XD\n",
      "Hook Hookio\n",
      "Krishanu Shit\n",
      "Suman Shit\n",
      "Debendra Nath  Shit\n",
      "basavaraju  bc\n",
      "santanu shit\n",
      "Vinit jaat\n",
      "Tapas  Shit\n",
      "Omg Rai\n",
      "Tapas Sheet\n",
      "Gopal Shit\n",
      "Mohammed Ali  General\n",
      "sahil shet\n",
      "Ujjal Shit\n",
      "Biswajit  Shit\n",
      "John Ho\n",
      "SUDESH  SHIT\n",
      "writtik shit\n",
      "Sukdeb Sheet\n",
      "KUNTAL SHEET\n",
      "subhash jaat\n",
      "Manik Sheet\n",
      "SNEHASISH SHIT\n",
      "WHITE WOLF_YT ALL GAMES\n",
      "Manik Sheet\n",
      "White Devil\n",
      "Manik Shit\n",
      "Apu  Shit\n",
      "MANOJIT SHIT\n",
      "Haramohan Shit\n",
      "Samir Shit\n",
      "Samir Shit\n",
      "Ass Reddy\n",
      "Suva Shit\n",
      "Manish  joon\n",
      "Omg Tamizhaa\n",
      "Sumati  Shit\n",
      "Suman Shit\n",
      "Kamal Deep.U\n",
      "169.38316893577576\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in names:\n",
    "    out = test(i)\n",
    "    bool, booli = False, True\n",
    "    # if (out==1):\n",
    "    #     print(i)\n",
    "    for k in i.split():\n",
    "        if (k.lower() in ex):\n",
    "            booli = False\n",
    "        if (k in hi):\n",
    "            bool = True\n",
    "    if ((out and booli) or bool):\n",
    "        print(i)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dcb1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
